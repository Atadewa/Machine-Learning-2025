{"cells":[{"cell_type":"markdown","metadata":{"id":"KT2qmk6r5MNp"},"source":["Nama : Aditya Atadewa  \n","Kelas : TI 3G  \n","NIM : 2341720174  \n","Absen : 01  "]},{"cell_type":"markdown","metadata":{"id":"8f2d9a28"},"source":["# Praktikum 6\n","\n","Melakukan percobaan penggunaan ANNOY, FAISS, dan HNSWLIB pada dataset sekunder berukuran besar (Micro Spotify) pada link berikut: https://www.kaggle.com/datasets/bwandowando/spotify-songs-with-attributes-and-lyrics/data."]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":78876,"status":"ok","timestamp":1761014061580,"user":{"displayName":"Aditya Atadewa","userId":"07837304634321480627"},"user_tz":-420},"id":"3bc0f8ca","outputId":"0cdb56b2-1f72-4ebf-be7b-e40000eb6198"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/647.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[32m450.6/647.5 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m645.1/647.5 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m647.5/647.5 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for annoy (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for hnswlib (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"]}],"source":["!pip install -q annoy faiss-cpu hnswlib"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5274,"status":"ok","timestamp":1761014441796,"user":{"displayName":"Aditya Atadewa","userId":"07837304634321480627"},"user_tz":-420},"id":"A0hQJrTVo3D2","outputId":"c00feccc-dc5d-4126-b450-50d3a6b1ca29"},"outputs":[{"name":"stdout","output_type":"stream","text":["Using Colab cache for faster access to the 'spotify-songs-with-attributes-and-lyrics' dataset.\n"]}],"source":["import kagglehub\n","\n","path = kagglehub.dataset_download(\"bwandowando/spotify-songs-with-attributes-and-lyrics\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"17fa3030"},"outputs":[{"name":"stdout","output_type":"stream","text":["Exact NN (queries=1000) done in 3798.416 s\n","Annoy build: 75.093 s, query all: 253.940 s\n","HNSW build: 165.886 s, query all: 137.792 s\n","FAISS build: 0.426 s, query all: 717.884 s\n","\n","Summary (build time | query time for sampled points | recall@k)\n","Exact:  - | 3798.416 s (queries only) | recall=1.00\n","Annoy:  75.093 s | 253.940 s | recall@10=0.9945\n","HNSW:   165.886 s | 137.792 s | recall@10=0.9955\n","FAISS:  0.426 s | 717.884 s | recall@10=0.9982\n","\n","Top-5 neighbors for first sampled query (dataset index = 287796)\n","Exact NN: [     0 394553 764272 837727 749223]\n","Annoy:    [0, 394553, 764272, 837727, 749223]\n","HNSW:     [     0 394553 764272 837727 749223]\n","FAISS:    [     0 394553 764272 837727 749223]\n"]}],"source":["import pandas as pd\n","import numpy as np\n","import os\n","import time\n","import faiss\n","from annoy import AnnoyIndex\n","import hnswlib\n","from sklearn.neighbors import NearestNeighbors\n","from joblib import Parallel, delayed\n","from sklearn.preprocessing import StandardScaler\n","\n","# Use all available CPU cores where possible\n","n_cores = os.cpu_count() or 1\n","os.environ.setdefault('OMP_NUM_THREADS', str(n_cores))\n","os.environ.setdefault('OPENBLAS_NUM_THREADS', str(n_cores))\n","os.environ.setdefault('MKL_NUM_THREADS', str(n_cores))\n","# Tell faiss to use multiple threads (if built with OpenMP)\n","try:\n","    faiss.omp_set_num_threads(n_cores)\n","except Exception:\n","    pass\n","\n","# -------------------------------\n","# Load dataset (drop NaNs in chosen features)\n","# -------------------------------\n","df = pd.read_csv(f'{path}/songs_with_attributes_and_lyrics.csv')\n","\n","features = ['danceability', 'energy', 'loudness', 'speechiness',\n","            'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo']\n","df = df[features].dropna().reset_index(drop=True)\n","X = df.values\n","\n","# Standardize and cast to float32 (required by faiss/hnswlib)\n","scaler = StandardScaler()\n","X_scaled = scaler.fit_transform(X).astype(np.float32)\n","\n","n = X_scaled.shape[0]\n","k = 10  # jumlah nearest neighbors\n","# To keep this runnable on limited RAM, sample up to 1000 query points\n","n_queries = min(1000, n)\n","rng = np.random.default_rng(42)\n","query_idx = rng.choice(n, size=n_queries, replace=False)\n","# Xq = X_scaled[query_idx]\n","Xq = X_scaled\n","\n","# -------------------------------\n","# Exact Nearest Neighbor (brute-force) - only for the sampled queries\n","# -------------------------------\n","t0 = time.time()\n","nn = NearestNeighbors(n_neighbors=k, algorithm='brute', metric='euclidean', n_jobs=-1)\n","nn.fit(X_scaled)\n","dist_exact, idx_exact = nn.kneighbors(Xq)\n","time_exact = time.time() - t0\n","print(f\"Exact NN (queries={n_queries}) done in {time_exact:.3f} s\")\n","\n","# -------------------------------\n","# Annoy (build + query on sampled points)\n","# -------------------------------\n","t0 = time.time()\n","fdim = X_scaled.shape[1]\n","index_annoy = AnnoyIndex(fdim, 'euclidean')\n","for i, v in enumerate(X_scaled):\n","    index_annoy.add_item(i, v.tolist())\n","n_trees = 50\n","index_annoy.build(n_trees)\n","t_build_annoy = time.time() - t0\n","\n","tq = time.time()\n","# Annoy: parallelize queries using joblib (threading) to utilize multiple cores\n","def _query_annoy(v):\n","    return index_annoy.get_nns_by_vector(v.tolist(), k)\n","idx_annoy = Parallel(n_jobs=n_cores, prefer='threads')(delayed(_query_annoy)(v) for v in Xq)\n","time_query_annoy = time.time() - tq\n","print(f\"Annoy build: {t_build_annoy:.3f} s, query all: {time_query_annoy:.3f} s\")\n","\n","# -------------------------------\n","# HNSW (hnswlib)\n","# -------------------------------\n","t0 = time.time()\n","p = hnswlib.Index(space='l2', dim=fdim)\n","p.init_index(max_elements=n, ef_construction=200, M=16)\n","p.add_items(X_scaled)\n","p.set_ef(200)\n","t_build_hnsw = time.time() - t0\n","\n","tq = time.time()\n","# hnswlib supports num_threads in knn_query\n","idx_hnsw, dist_hnsw = p.knn_query(Xq, k=k, num_threads=n_cores)\n","time_query_hnsw = time.time() - tq\n","print(f\"HNSW build: {t_build_hnsw:.3f} s, query all: {time_query_hnsw:.3f} s\")\n","\n","# -------------------------------\n","# FAISS IVF (train on full set, query sampled points)\n","# -------------------------------\n","t0 = time.time()\n","quantizer = faiss.IndexFlatL2(fdim)\n","nlist = 100\n","index_faiss = faiss.IndexIVFFlat(quantizer, fdim, nlist, faiss.METRIC_L2)\n","# FAISS requires float32 and contiguous arrays\n","index_faiss.train(np.ascontiguousarray(X_scaled))\n","index_faiss.add(np.ascontiguousarray(X_scaled))\n","index_faiss.nprobe = 10\n","t_build_faiss = time.time() - t0\n","\n","tq = time.time()\n","# FAISS can use multiple threads via set_num_threads if available\n","try:\n","    faiss.omp_set_num_threads(n_cores)\n","except Exception:\n","    pass\n","D_faiss, idx_faiss = index_faiss.search(np.ascontiguousarray(Xq), k)\n","time_query_faiss = time.time() - tq\n","print(f\"FAISS build: {t_build_faiss:.3f} s, query all: {time_query_faiss:.3f} s\")\n","\n","# -------------------------------\n","# Evaluate recall@k for each ANN vs exact\n","# -------------------------------\n","def recall_at_k(true_idx, pred_idx, k):\n","    # true_idx: (n_queries, k), pred_idx: iterable of length n_queries with lists/arrays\n","    total = 0.0\n","    n = len(true_idx)\n","    for t, p in zip(true_idx, pred_idx):\n","        pset = set(p.tolist() if hasattr(p, 'tolist') else p)\n","        total += len(pset.intersection(set(t[:k]))) / float(k)\n","    return total / n\n","\n","rec_annoy = recall_at_k(idx_exact, idx_annoy, k)\n","rec_hnsw = recall_at_k(idx_exact, idx_hnsw, k)\n","rec_faiss = recall_at_k(idx_exact, idx_faiss, k)\n","\n","print('\\nSummary (build time | query time for sampled points | recall@k)')\n","print(f\"Exact:  - | {time_exact:.3f} s (queries only) | recall=1.00\")\n","print(f\"Annoy:  {t_build_annoy:.3f} s | {time_query_annoy:.3f} s | recall@{k}={rec_annoy:.4f}\")\n","print(f\"HNSW:   {t_build_hnsw:.3f} s | {time_query_hnsw:.3f} s | recall@{k}={rec_hnsw:.4f}\")\n","print(f\"FAISS:  {t_build_faiss:.3f} s | {time_query_faiss:.3f} s | recall@{k}={rec_faiss:.4f}\")\n","\n","# show top-5 neighbors for the first sampled query (original dataset index)\n","qid = query_idx[0]\n","print(\"\\nTop-5 neighbors for first sampled query (dataset index = {})\".format(int(qid)))\n","print(f\"Exact NN: {idx_exact[0][:5]}\")\n","print(f\"Annoy:    {idx_annoy[0][:5]}\")\n","print(f\"HNSW:     {idx_hnsw[0][:5]}\")\n","print(f\"FAISS:    {idx_faiss[0][:5]}\")"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyOXN4V0U/ML1muSEQtugrNS","name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}